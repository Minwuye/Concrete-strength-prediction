# üèóÔ∏è Concrete Compressive Strength Prediction

#This notebook builds a machine learning regression pipeline to predict **concrete compressive strength (MPa)** based on mix design features.

#It supports:
#- Data loading from user-provided Excel files
#- Cleaning and preprocessing
#- Handling imbalance in target values with **SMOGN** oversampling
#- Training and tuning multiple regression models
#- Evaluating results with residual analysis and actual vs predicted plots
#- Interpreting models with **SHAP**

#> **Instructions:**  
#> 1Ô∏è‚É£ Upload your own Excel dataset.  
#> 2Ô∏è‚É£ Update the data path in the cell below.  

# 1. load all the important modules and libraries
# Data manipulation
import pandas as pd
import numpy as np

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Models
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import StackingRegressor, VotingRegressor

#fine tuning
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Interpretability
import shap

# oversampling
import smogn

# Utilities
from pathlib import Path

# 2. load the traning dataset and Visualization
# load the data

data_path = Path(r"Dataset path / "dataset.xlsx"
data = pd.read_excel(data_path)
data.head()

print(data.shape)
print(data.columns)
data.describe()

# correlation Plots heat map

sns.pairplot(data)
plt.show()

plt.figure(figsize=(10,8))
sns.heatmap(data.corr(), annot=True, cmap='coolwarm')
plt.show()

# clean the columns
print(data.columns.tolist())
data.columns = data.columns.str.strip()
data.columns = data.columns.str.replace('\n', ' ').str.strip()

print(data.columns.tolist())

# Histogram plots
sns.histplot(data['Concrete compressive strength (MPa)'], bins=30, kde=True)
plt.title('Distribution of Compressive Strength')
plt.show()

print(data['Concrete compressive strength (MPa)'].quantile([0.25, 0.5, 0.75]))

X = data.drop('Concrete compressive strength (MPa)', axis=1)
y = data['Concrete compressive strength (MPa)']

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Binning target for stratification
y_binned = pd.qcut(y, q=3, labels=['low', 'medium', 'high'])

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, stratify=y_binned, random_state=42
)

# Combine scaled X and y into one dataframe for smogn
df_train = pd.DataFrame(X_train, columns=X.columns)
df_train['Concrete compressive strength (MPa)'] = y_train.values

# Apply for SMOGN oversampling 

df_smogn = smogn.smoter(
    data=df_train,
    y='Concrete compressive strength (MPa)'
)

X_train_smogn = df_smogn.drop('Concrete compressive strength (MPa)', axis=1)
y_train_smogn = df_smogn['Concrete compressive strength (MPa)']

# üìå Clean column names
data.columns = data.columns.str.replace('\n', ' ').str.strip()

sns.histplot(data['Concrete compressive strength (MPa)'], bins=30, kde=True)
plt.title('Target Distribution Before Oversampling')
plt.show()

# üìå Apply SMOGN

data_oversampled = smogn.smoter(
    data = data,
    y = 'Concrete compressive strength (MPa)'
)

print("‚úÖ Data shape before:", data.shape)
print("‚úÖ Data shape after:", data_oversampled.shape)

# üìå Check new distribution
sns.histplot(data_oversampled['Concrete compressive strength (MPa)'], bins=30, kde=True)
plt.title('Target Distribution After Oversampling')
plt.show()


# define models

models = {
    'LinearRegression': LinearRegression(),
    'RandomForest': RandomForestRegressor(n_estimators=100, random_state=42),
    'GradientBoosting': GradientBoostingRegressor(random_state=42),
    'MLPRegressor': MLPRegressor(hidden_layer_sizes=(64,64), max_iter=500, random_state=42)
}

# training and evaluation metrics

def evaluate_model(name, model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    mae = mean_absolute_error(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)
    print(f"{name}: MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.2f}")
    return model

# model training

print("‚úÖ Without Oversampling")
trained_models = {}

for name, model in models.items():
    trained_models[name] = evaluate_model(name, model, X_train, y_train, X_test, y_test)

print("\n‚úÖ With SMOGN Oversampling")
trained_models_smogn = {}

for name, model in models.items():
    trained_models_smogn[name] = evaluate_model(name, model, X_train_smogn, y_train_smogn, X_test, y_test)

# interperatbililty

best_model = trained_models_smogn['RandomForest']

explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test)

# Feature names
feature_names = X.columns

shap.summary_plot(shap_values, X_test, feature_names=feature_names)

shap.dependence_plot(0, shap_values, X_test, feature_names=feature_names)



# hyperparameter tuning

rf_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

gb_grid = {
    'n_estimators': [100, 200],
    'learning_rate': [0.01, 0.1],
    'max_depth': [3, 5]
}

svr_grid = {
    'C': [0.1, 1, 10],
    'epsilon': [0.1, 0.2],
    'kernel': ['rbf', 'linear']
}

mlp_grid = {
    'hidden_layer_sizes': [(50,), (100,), (50, 50)],
    'activation': ['relu', 'tanh'],
    'alpha': [0.0001, 0.001]
}

param_grids = {
    'RandomForest': rf_grid,
    'GradientBoosting': gb_grid,
    'SVR': svr_grid,
    'MLP': mlp_grid
}

#tuning
def tune_model(model, param_grid, X_train, y_train):
    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=3,
        scoring='neg_mean_squared_error',
        n_jobs=-1
    )
    grid.fit(X_train, y_train)
    print(f"‚úÖ Best Params: {grid.best_params_}")
    return grid.best_estimator_

best_rf = tune_model(RandomForestRegressor(), rf_grid, X_train, y_train)

models = {
    'LinearRegression': LinearRegression(),
    'Ridge': Ridge(),
    'Lasso': Lasso(),
    'RandomForest': RandomForestRegressor(),
    'GradientBoosting': GradientBoostingRegressor(),
    'ExtraTrees': ExtraTreesRegressor(),
    'KNeighbors': KNeighborsRegressor(),
    'SVR': SVR(),
    'MLP': MLPRegressor(max_iter=500)
}

best_models = {}

for name, model in models.items():
    if name in param_grids:
        print(f"üîç Tuning {name}...")
        best_models[name] = tune_model(model, param_grids[name], X_train, y_train)
    else:
        print(f"‚úÖ Training {name} without tuning...")
        model.fit(X_train, y_train)
        best_models[name] = model

for name, model in best_models.items():
    preds = model.predict(X_test)
    mae = mean_absolute_error(y_test, preds)
    rmse = np.sqrt(mean_squared_error(y_test, preds))
    r2 = r2_score(y_test, preds)
    print(f"{name}: MAE={mae:.2f}, RMSE={rmse:.2f}, R2={r2:.2f}")

estimators = [
    ('rf', best_models['RandomForest']),
    ('gb', best_models['GradientBoosting']),
    ('svr', best_models['SVR'])
]

stack = StackingRegressor(
    estimators=estimators,
    final_estimator=Ridge()
)

stack.fit(X_train, y_train)
stack_preds = stack.predict(X_test)

print("Stacked Model RMSE:", np.sqrt(mean_squared_error(y_test, stack_preds)))

vote = VotingRegressor(estimators=estimators)
vote.fit(X_train, y_train)
vote_preds = vote.predict(X_test)
print("Voting Model RMSE:", np.sqrt(mean_squared_error(y_test, vote_preds)))

#plotting
# Calculate residuals
y_pred = best_model.predict(X_test)
residuals = y_test - y_pred

# Residual plot
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_pred, y=residuals)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted')
plt.ylabel('Residuals')
plt.title('Residual Plot')
plt.show()

# Actual vs Predicted
plt.figure(figsize=(8, 6))
sns.scatterplot(x=y_test, y=y_pred)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Actual vs Predicted')
plt.show()


